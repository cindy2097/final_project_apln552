Raw Text of “Speech and Language Processing” by Jurafsky & Martin (12/30/2020 Draft)

Chapter 2: Regular Expressions, Text Normalization, Edit Distance

2.0    Introduction
The dialogue above is from ELIZA, an early natural language processing system that could carry on a limited conversation  with a user by imitating the responses of a Rogerian  psychotherapist (Weizenbaum,  1966).  ELIZA  is a surprisingly  simple program that uses pattern matching to recognize phrases like “I need X” and translate them  into suitable outputs  like  “What  would  it mean  to  you  if  you  got X?”.  This simple technique  succeeds  in this domain because ELIZA doesn't  actually  need to know anything to mimic a Rogerian  psychotherapist.  As Weizenbaum  notes, this is one of the few dialogue genres where listeners can act as if they know nothing of the world.   Eliza's  mimicry  of  human  conversation  was  remarkably  successful:  many people  who interacted  with ELIZA came to believe  that it really  understood  them and  their  problems,  many  continued  to  believe  in  ELIZA's  abilities  even  after  the program's  operation  was explained  to  them  (Weizenbaum,  1976),  and  even  today such chatbots are a fun diversion. Of  course  modern  conversational agents  are much  more than  a diversion;  they can answer questions, book flights, or find restaurants, functions for which they rely on a much  more sophisticated  understanding  of  the user's intent,  as we will see in Chapter  24. Nonetheless,  the simple  pattern-based  methods  that  powered  ELIZA and other chatbots play a crucial role in natural language processing. We'll begin with the most important tool for describing text patterns:  the regular expression.   Regular  expressions  can  be used to specify  strings  we might  want  to extract from a document, from transforming  “I need X” in Eliza above, to defining strings like $199 or $24.99 for extracting tables of prices from a document. We'll then turn to a set of tasks collectively called text normalization, in which regular  expressions  play  an  important  part.   Normalizing  text  means  converting  it to a  more  convenient,  standard  form.  For example,  most of  what  we are going  to do  with  language  relies  on  first  separating  out  or  tokenizing  words  from  running text,  the task  of  tokenization.   English  words  are often  separated  from  each other by  whitespace,  but  whitespace is not always sufficient.  New York  and  rock ’n’  roll are sometimes treated as large words despite the fact that they contain spaces, while sometimes  we'll  need to separate I’m into the two words  I and am.  For processing tweets or texts we'll need to tokenize emoticons like :) or hashtags like #nlproc. Some languages, like Japanese, don't have spaces between words, so word tokenization becomes more difficult. Another  part  of  text  normalization  is  lemmatization,  the  task  of  determining that two words have the same root, despite their surface differences.  For example, the  words  sang, sung, and  sings  are forms  of  the  verb  sing.  The  word  sing  is the common  lemma  of  these words,  and  a lemmatizer  maps  from all of  these to sing. Lemmatization  is essential  for processing  morphologically complex  languages  like Arabic.  Stemming refers to a simpler version of lemmatization in which we mainly just strip suffixes  from the end of  the word.  Text normalization  also includes sentence segmentation:  breaking  up a text into individual  sentences,  using cues  like periods or exclamation  points. Finally, we'll need to compare words and other strings.  We'll introduce a metric called edit distance that measures how similar two strings are based on the number of  edits  (insertions,  deletions,  substitutions) it  takes  to  change  one string  into  the other.  Edit distance is an algorithm  with applications  throughout language processing, from spelling correction  to speech recognition  to coreference resolution.

2.1    Regular Expressions
One of  the  unsung  successes  in  standardization in  computer  science  has  been  the regular expression  (RE), a language for specifying text search  strings.  This practical language  is  used  in  every  computer  language,  word  processor,  and  text  processing  tools  like the Unix tools  grep or Emacs.  Formally,  a regular  expression  is an algebraic  notation for characterizing a set of  strings.  They  are particularly  useful for  searching  in  texts,  when  we have  a pattern  to search  for  and  a corpus of texts to search through.  A regular expression search function will search through the corpus,  returning  all texts that match  the pattern.  The corpus can  be a single document or a collection. For example, the Unix command-line tool grep takes a regular expression and returns every line of the input document that matches the expression. A search can be designed to return every match on a line, if there are more than one,  or just  the first  match.  In  the following  examples  we generally  underline  the exact part of the pattern  that matches the regular expression  and show only the first match. We'll show regular expressions delimited by slashes but note that slashes are not part of the regular expressions. Regular expressions come in many variants. We'll be describing extended regular  expressions;  different regular expression  parsers may only recognize subsets of these, or treat some expressions slightly differently.  Using an online regular expression tester is a handy way to test out your expressions and explore these variations.

2.1.1    Basic Regular Expression Patterns
The simplest kind of regular expression is a sequence of simple characters.  To search for woodchuck, we type /woodchuck/.  The expression  /Buttercup/  matches any string containing the substring Buttercup; grep with that expression would return the line  I'm called  little  Buttercup.  The search  string can consist  of  a single character (like / ! /)  or a sequence of characters (like /urgl/). Regular  expressions  are case sensitive;  lower  case  /s/    is distinct  from  upper case  /S/   (/s/  matches  a  lower  case  s  but  not  an  upper  case  S). This  means  that the pattern /woodchucks/  will not match the string Woodchucks. We can solve this problem with the use of the square braces [ and ]. The string of characters inside the braces specifies a disjunction of characters to match. For example, Fig. 2.2 shows that the pattern /[wW]/ matches patterns containing either w or W. The regular expression  /[1234567890]/  specifies any single digit.  While such classes of characters as digits or letters are important building blocks in expressions, they can get awkward (e.g., it's inconvenient to specify /[ABCDEFGHIJKLHNOPQRSTUVHXYZ]/ to mean “any capital letter”).  In cases where there is a well-defined  sequence associated with a set of characters,  the brackets can be used with the dash (-)  to specify any one character in a range. The pattern  /[2-5]/  specifies any one of the characters 2, 3, 4, or 5. The pattern /[b-g]/  specifies one of the characters b, c, d, e, f, or g. Some other examples are shown in Fig. 2.3. The square braces can also be used to specify what a single character cannot be, by use of the caret ^. If the caret  ^  is the first symbol after the open square brace, the resulting pattern is negated.  For example, the pattern /[^a]/  matches any single character  (including  special characters)  except  a.  This is only  true when  the caret is the first symbol after the open square brace.  If it occurs anywhere else, it usually stands for a caret; Fig. 2.4 shows some examples.How can we talk about optional elements,  like an optional  s in woodchuck  and woodchucks. We can't use the square brackets,  because while they allow  us to say “s or S”, they don't allow us to say “s or nothing”.  For this we use the question mark /?/, which means “the preceding character or nothing”, as shown in Fig. 2.5. We  can  think  of  the  question  mark  as  meaning  “zero  or  one  instances  of  the previous character”.  That is, it's a way of specifying  how many  of  something  that we  want,  something  that  is  very  important  in  regular  expressions.   For  example, consider  the language of   certain  sheep,  which consists of  strings that look like the following: baa! baaa! baaaa! Baaaaa! This language consists of strings with a b, followed by at least two a's, followed by an exclamation  point. The set of operators that allows us to say things like “some number of as”  are based on the asterisk or *, commonly  called the Kleene * (generally pronounced “cleany star”). The Kleene star means “zero or more occurrences of the immediately  previous character or regular expression”.  So /a*/  means “any string of  zero or more as”.  This will match a or aaaaaa, but it will also match Off Minor since the string OffMinor has zero a's. So the regular expression for matching one or more a is /aa*/, meaning one a followed by zero or more as. More complex patterns  can  also  be  repeated.   So  /[ab]*/   means  “zero  or  more a's  or b's”  (not “zero or more right square braces”).  This will match strings like aaaa or ababab or aabb. For specifying multiple digits (useful for finding prices) we can extend /[0-9]/, the  regular  expression  for  a  single  digit.    An  integer  (a  string  of  digits)  is  thus /[0-9][0-9]*/.  (Why isn't it just /[0-9]*/?) Sometimes it's annoying to have to write the regular expression for digits twice, so  there  is  a  shorter  way  to specify  “at  least  one”  of  some  character.   This  is  the Kleene  +,  which  means  “one  or  more  occurrences  of  the  immediately  preceding character or regular expression”.  Thus, the expression  /[0-9]+/  is the normal way to  specify  “a  sequence  of  digits”.   There  are  thus  two  ways  to  specify  the  sheep language:  /baaa*!/  or /baa+!/. One very important special character is the period (/./),  a wildcard expression that matches any single character (except a carriage return), as shown in Fig. 2.6. The wildcard  is often used together  with  the Kleene star to mean “any string of characters”. For example,  suppose  we  want  to find  any  line in  which  a particular word,  for  example, aardvark, appears  twice.  We can  specify  this  with  the regular expression  /aardvark .* aardvark/. Anchors are special characters that anchor regular expressions to particular places in a string. The most common anchors are the caret ^ and the dollar sign $. The caret  ^  matches the start of a line.  The pattern  /^The/  matches the word “The” only at the start of  a line.  Thus,  the caret  ^  has three  uses:  to match  the start of  a line,  to indicate a negation inside of square brackets, and just to mean a caret.  (What are the contexts that allow grep or Python to know which function a given caret is supposed to have?)  The dollar sign  $ matches the end of a line.  So the pattern $ is a useful pattern  for matching  a space at  the end  of  a line,  and  /^The  dog\.$/  matches  a line that contains only the phrase The dog. (We have to use the backslash here since we want the . to mean “period” and not the wildcard.) There are also two other anchors:  \b  matches a word boundary, and \B matches a non-boundary. Thus,  /\bthe\b/  matches  the word  the  but  not the word  other. More technically, a “word” for the purposes of a regular expression is defined as any sequence of digits, underscores, or letters; this is based on the definition of “words” in  programming  languages. For  example,  /\b99\b/  will  match  the  string  99  in There are 99 bottles of beer on the wall (because 99 follows a space) but not 99 in There  are  299 bottles of beer on the wall (since 99 follows a number).  But it will match 99 in $99 (since 99 follows a dollar sign ($), which is not a digit, underscore, or letter).